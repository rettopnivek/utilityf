% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/utility_functions.R
\name{infoCrit}
\alias{infoCrit}
\title{Calculate Select Information Criterion Values}
\usage{
infoCrit(logLik, k, n, type = "AICc")
}
\arguments{
\item{logLik}{a log-likelihood value.}

\item{k}{the number of free parameters for the model.}

\item{n}{the number of observations in the sample.}

\item{type}{indicates whether the 'AICc' or 'BIC' value should be returned.}
}
\value{
A value for either the AICc or the BIC.
}
\description{
A function that calculates either Akaike's Information Criterion (AIC)
with a correction or the Bayesian Information Criterion (BIC).
}
\details{
Given a summed log-likelihood \eqn{L} from a model and \eqn{K} free
  parameters, the AIC is \deqn{ 2K - 2L. } A correction for finite samples is
  recommended (e.g. Burnham & Anderson, 2002), and for \eqn{N} observations
  the new equation is \deqn{ 2K - 2L + \frac{2K(K+1)}{N+K+1}. } The formula
  for the BIC is \deqn{ log(N)K - 2L. } For both criterions, models with
  smaller values are to be preferred.
}
\examples{
N = 100; K = 2
x = rnorm( 100, 1, 2 )
m1 = sum( dnorm( x, 0, 1, log = T ) )
m2 = sum( dnorm( x, 1, 2, log = T ) )
# AIC values comparing the two models
print( round( infoCrit( c(m1,m2), K, N ), 2 ) )
# BIC values comparing the two models
print( round( infoCrit( c(m1,m2), K, N ), 2 ), type = 'BIC' )
}
\references{
Akaike, H. (1973). Information theory and an extension of the maximum likelihood
  principle. In B. N. Petrov & F. Caski (Eds.), Proceedings of the Second
  International Symposium on Information Theory (pp. 267-281). Budapest:Akademiai
  Kiado.

Burnham, K. P., & Anderson, D. R. (2002). Model selection and multimodel inference:
  A practical information-theoretic approach. New York: Springer-Verlag.

Schwarz, G. (1978). Estimating the dimension of a model. Annals of Statistics, 6,
  461-464.
}

